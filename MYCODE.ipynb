{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29af2f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all the necessary libraries\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import imutils\n",
    "import os\n",
    "from scipy.spatial import distance as dist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ec251fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['person', 'bicycle', 'car', 'motorbike', 'aeroplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'sofa', 'pottedplant', 'bed', 'diningtable', 'toilet', 'tvmonitor', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"yolo-coco\"\n",
    "\n",
    "# initializing minimum probability to filter weak detections \n",
    "MIN_CONF = 0.3\n",
    "# threshold when applying non-maxim suppression\n",
    "NMS_THRESH = 0.3\n",
    "\n",
    "# we will now define the minimum distance for social distancing\n",
    "MIN_DISTANCE = 70\n",
    "\n",
    "labelsPath = os.path.sep.join([MODEL_PATH, \"coco.names\"])\n",
    "LABELS = open(labelsPath).read().strip().split(\"\\n\")\n",
    "\n",
    "print(LABELS)\n",
    "\n",
    "print(len(LABELS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9ead2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, deriving the paths to the YOLO weights and model configuration\n",
    "weightsPath = os.path.sep.join([MODEL_PATH, \"yolov3.weights\"])\n",
    "configPath = os.path.sep.join([MODEL_PATH, \"yolov3.cfg\"])\n",
    "\n",
    "# next step is loading our YOLO object detector trained on COCO dataset (80 classes)\n",
    "net = cv.dnn.readNetFromDarknet(configPath, weightsPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6e34c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS = []\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    \n",
    "    parser.add_argument('-w', '--weights',\n",
    "        default='./yolo-coco/yolov3.weights')\n",
    "\n",
    "    parser.add_argument('-cfg', '--config',\n",
    "        default='./yolo-coco/yolov3.cfg')\n",
    "\n",
    "    parser.add_argument('-v', '--video-path',\n",
    "        default='Test Video.mp4')\n",
    "\n",
    "    parser.add_argument('-vo', '--video-output-path',\n",
    "        default='output_file.avi')\n",
    "\n",
    "    parser.add_argument('-d', '--display', \n",
    "        default=1)\n",
    "    parser.add_argument('-l', '--labels',\n",
    "        default='./yolo-coco/coco.names')\n",
    "\n",
    "\n",
    "    FLAGS, unparsed = parser.parse_known_args()\n",
    "\n",
    "\n",
    "# Get the labels\n",
    "    LABELS = open(FLAGS.labels).read().strip().split('\\n')\n",
    "\n",
    "# Load the weights and configutation to form the pretrained YOLOv3 model\n",
    "    net = cv.dnn.readNetFromDarknet(FLAGS.config, FLAGS.weights)\n",
    "\n",
    "    ln = net.getLayerNames()\n",
    "    ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "if FLAGS.video_path:\n",
    "# initialize the video stream to output video file\n",
    "# open input video if available else webcam stream\n",
    "    vs = cv.VideoCapture(FLAGS.video_path if FLAGS.video_path else 0)\n",
    "    writer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3ffe376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_people(frame, net, ln, personIdx=0):\n",
    "    # grab dimensions of the frame and initialize the list of results\n",
    "    (H, W) = frame.shape[:2]\n",
    "    results = []\n",
    "\n",
    "    # construct a blob from the input frame and then perfrom a forward pass of the YOLO object detector, giving us the bounding boxes and associated probabilities\n",
    "    blob = cv.dnn.blobFromImage(frame, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    layerOutputs = net.forward(ln)\n",
    "\n",
    "    # initializing the  lists of detected bounding boxes, centroids, and confidences\n",
    "    boxes = []\n",
    "    centroids = []\n",
    "    confidences = []\n",
    "\n",
    "    # looping over each of the layer outputs\n",
    "    for output in layerOutputs:\n",
    "        # looping over each of the detections\n",
    "        for detection in output:\n",
    "            # extracting the class ID and probability of the current object detection\n",
    "            scores = detection[5:]\n",
    "            classID = np.argmax(scores)\n",
    "            confidence = scores[classID]\n",
    "\n",
    "            # filter detections \n",
    "            if classID == personIdx and confidence > MIN_CONF:\n",
    "                # scale the bounding box coordinates back relative to the size of the image, keeping in mind that YOLO actually returns the center (x, y)-coordinatesof the bounding box followed by the boxes' width and height\n",
    "                box = detection[0:4] * np.array([W, H, W, H])\n",
    "                (centerX, centerY, width, height) = box.astype(\"int\")\n",
    "\n",
    "                x = int(centerX - (width / 2))\n",
    "                y = int(centerY - (height / 2))\n",
    "\n",
    "                # update the list of bounding box coordinates, centroids and confidences\n",
    "                boxes.append([x, y, int(width), int(height)])\n",
    "                centroids.append((centerX, centerY))\n",
    "                confidences.append(float(confidence))\n",
    "\n",
    "    # here applying the non-maxima suppression to suppress weak, overlapping bounding boxes\n",
    "    idxs = cv.dnn.NMSBoxes(boxes, confidences, MIN_CONF, NMS_THRESH)\n",
    "\n",
    "    # ensuring at least 1 detection exists\n",
    "    if len(idxs) > 0:\n",
    "        # looping over the indexes\n",
    "        for i in idxs.flatten():\n",
    "            # extract the bounding box coordinates\n",
    "            (x, y) = (boxes[i][0], boxes[i][1])\n",
    "            (w, h) = (boxes[i][2], boxes[i][3])\n",
    "\n",
    "            r = (confidences[i], (x, y, x + w, y + h), centroids[i])\n",
    "            results.append(r)\n",
    "\n",
    "    # return the list of results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6833ee99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Writing stream to output\n",
      "--> Writing stream to output\n",
      "--> Writing stream to output\n",
      "--> Writing stream to output\n",
      "--> Writing stream to output\n",
      "--> Writing stream to output\n",
      "--> Writing stream to output\n",
      "--> Writing stream to output\n",
      "--> Writing stream to output\n",
      "--> Writing stream to output\n",
      "--> Writing stream to output\n",
      "--> Writing stream to output\n",
      "--> Writing stream to output\n",
      "--> Writing stream to output\n",
      "--> Writing stream to output\n",
      "--> Writing stream to output\n",
      "--> Writing stream to output\n",
      "--> Writing stream to output\n",
      "--> Writing stream to output\n",
      "--> Writing stream to output\n",
      "--> Writing stream to output\n",
      "--> Writing stream to output\n",
      "--> Writing stream to output\n",
      "--> Writing stream to output\n",
      "--> Writing stream to output\n",
      "--> Writing stream to output\n",
      "--> Writing stream to output\n",
      "--> Writing stream to output\n",
      "--> Writing stream to output\n",
      "--> Writing stream to output\n",
      "--> Writing stream to output\n",
      "--> Writing stream to output\n",
      "--> Writing stream to output\n",
      "--> Writing stream to output\n",
      "--> Writing stream to output\n",
      "--> Writing stream to output\n",
      "--> Writing stream to output\n",
      "--> Writing stream to output\n",
      "--> Writing stream to output\n",
      "--> Writing stream to output\n",
      "--> Writing stream to output\n",
      "--> Writing stream to output\n"
     ]
    }
   ],
   "source": [
    "# loop over the frames from the video stream\n",
    "while True:\n",
    "    # read the next frame from the input video\n",
    "    (grabbed, frame) = vs.read()\n",
    "\n",
    "    # if the frame was not grabbed then the stream will be ended\n",
    "    if not grabbed:\n",
    "        break\n",
    "    # to resize frame and detect people\n",
    "    frame = imutils.resize(frame, width=700)\n",
    "    results = detect_people(frame, net, ln, personIdx=LABELS.index(\"person\"))\n",
    "    violate = set()\n",
    "        # ensuring there are at least two people detection\n",
    "    if len(results) >= 2:\n",
    "        # extract all centroids from the results and compute the Euclidean distances\n",
    "        # between all pairs of the centroids\n",
    "        centroids = np.array([r[2] for r in results])\n",
    "        D = dist.cdist(centroids, centroids, metric=\"euclidean\")\n",
    "\n",
    "        # loop over the upper triangular of the distance matrix\n",
    "        for i in range(0, D.shape[0]):\n",
    "            for j in range(i+1, D.shape[1]):\n",
    "                # check to see if the distance between any two centroid pairs is less\n",
    "                # than the configured number of pixels\n",
    "                if D[i, j] < MIN_DISTANCE:\n",
    "                    # update the violation set with the indexes of the centroid pairs\n",
    "                    violate.add(i)\n",
    "                    violate.add(j)\n",
    "\n",
    "    # loop over the results\n",
    "    for (i, (prob, bbox, centroid)) in enumerate(results):\n",
    "        # extract teh bounding box and centroid coordinates, then initialize the color of the annotation\n",
    "        (startX, startY, endX, endY) = bbox\n",
    "        (cX, cY) = centroid\n",
    "        color = (0, 255, 0)\n",
    "\n",
    "        # if the index pair exists within the violation set, then update the color\n",
    "        if i in violate:\n",
    "            color = (0, 0, 255)\n",
    "\n",
    "        # draw (1) a bounding box around the person and (2) the centroid coordinates of the person\n",
    "        cv.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n",
    "        cv.circle(frame, (cX, cY), 5, color, 1)\n",
    "\n",
    "    # draw the total number of social distancing violations on the output frame\n",
    "    text = \"No. of People Violating : {}\".format(len(violate))\n",
    "    cv.putText(frame, text, (10, frame.shape[0] - 25), cv.FONT_HERSHEY_SIMPLEX, 0.8, (51, 51, 255), 3)\n",
    "\n",
    "    # check to see if the output frame should be displayed to the screen\n",
    "    if FLAGS.display > 0:\n",
    "        # show the output frame\n",
    "        cv.imshow(\"Eagle's Eye\", frame)\n",
    "        key = cv.waitKey(1) & 0xFF\n",
    "\n",
    "        # press 'q' to break from the loop\n",
    "        if key == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    # if  the video writer has not been  as none\n",
    "    if writer is None:\n",
    "        # initialize the video writer\n",
    "        fourcc = cv.VideoWriter_fourcc(*\"MJPG\")\n",
    "        writer = cv.VideoWriter(FLAGS.video_output_path, fourcc, 25, (frame.shape[1], frame.shape[0]), True)\n",
    "\n",
    "    if writer is not None:\n",
    "        print(\"--> Writing stream to output\")\n",
    "        writer.write(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d491827d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c77f5e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
